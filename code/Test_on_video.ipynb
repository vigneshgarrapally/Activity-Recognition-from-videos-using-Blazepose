{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnf1sdZa-FiA",
        "outputId": "4946a538-8a45-4817-870c-ba2cc9f9988e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY8_3p1HNG8W",
        "outputId": "fc2880d0-2b21-4ab2-af16-2f84dddc7f31"
      },
      "source": [
        "#Install required Dependencies\n",
        "!pip install mediapipe\n",
        "!pip install sk-video"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.7/dist-packages (0.8.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.36.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.15.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (20.3.0)\n",
            "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (56.0.0)\n",
            "Requirement already satisfied: sk-video in /usr/local/lib/python3.7/dist-packages (1.1.10)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sk-video) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sk-video) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx4ym0_ticge"
      },
      "source": [
        "#Import required modules\n",
        "import os\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense,Flatten,Dropout,LSTM,TimeDistributed\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f3w3EEj6mrP",
        "outputId": "d9598ce9-6550-4e85-8806-801df91a8bfe"
      },
      "source": [
        "#object for pose estimation\n",
        "mp_pose = mp.solutions.pose\n",
        "pose=mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
        "currvideo=np.zeros((33*4,))\n",
        "video_path=\"/content/drive/Shareddrives/Major project/Major Project/Human Activity Recognition on Yoga Dataset/Yoga dataset(6asanas)/Bhujangasana/Bhujangasana (8).mp4\"\n",
        "cam=cv.VideoCapture(video_path)     #open a video file through the given path\n",
        "print(video_path)\n",
        "z=0\n",
        "while(True):\n",
        "  ret,frame = cam.read()            #reading a video which returns the current frame and whether next frame is present or not\n",
        "  if ret:\n",
        "    if z%2==0:\n",
        "      arr=np.zeros((33,4))\n",
        "      results=pose.process(cv.cvtColor(frame, cv.COLOR_BGR2RGB))\n",
        "      if results.pose_landmarks is None:\n",
        "        continue\n",
        "      image_height, image_width, _ = frame.shape\n",
        "      j=0\n",
        "      for i in range(33):\n",
        "        arr[i,j] = results.pose_landmarks.landmark[i].x*image_width\n",
        "      j=1\n",
        "      for i in range(33):\n",
        "        arr[i,j] = results.pose_landmarks.landmark[i].y*image_height\n",
        "      j=2\n",
        "      for i in range(33):\n",
        "        arr[i,j] = results.pose_landmarks.landmark[i].z*image_width\n",
        "      j=3\n",
        "      for i in range(33):\n",
        "        arr[i][j] = results.pose_landmarks.landmark[i].visibility\n",
        "      min_x=min(arr[:,0])\n",
        "      max_x=max(arr[:,0])\n",
        "      min_y=min(arr[:,1])\n",
        "      max_y=max(arr[:,1])\n",
        "      j=0\n",
        "      for i in range(33):\n",
        "        arr[i,j]=(256/(max_x-min_x))*(arr[i][j]-min_x)\n",
        "      j=2\n",
        "      for i in range(33):\n",
        "        arr[i,j]=(256/(max_x-min_x))*(arr[i][j]-min_x)\n",
        "      j=1\n",
        "      for i in range(33):\n",
        "        arr[i,j]=(256/(max_y-min_y))*(arr[i][j]-min_y)\n",
        "      arr=arr.reshape(33*4,)\n",
        "      currvideo=np.vstack((currvideo,arr))\n",
        "  else:\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shareddrives/Major project/Major Project/Human Activity Recognition on Yoga Dataset/Yoga dataset(6asanas)/Bhujangasana/Bhujangasana (8).mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfV0GdjUPXwM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e3d616-683d-4927-a7b3-dd763361fbbe"
      },
      "source": [
        "\n",
        "\"/content/drive/Shareddrives/Major project/Major Project/Human Activity Recognition on Yoga Dataset/Yoga dataset(6asanas)/Bhujangasana/Bhujangasana (8).mp4\"\n",
        "currvideo=currvideo[1:,:]\n",
        "currvideo.shape\n",
        "currvideo.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1468, 132)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcf0Sp23lM3K"
      },
      "source": [
        "windowsize=64           #each windowsize represents a single input\n",
        "overlap_size=48         #overlap size is how many frames from each adjacent inputs should be common\n",
        "vid=currvideo\n",
        "testXfinal=np.zeros((1,64,132))\n",
        "for start in range(0,vid.shape[0]-windowsize, windowsize - overlap_size):\n",
        "  currvid=vid[start:start+windowsize,:]\n",
        "  currvid=currvid.reshape(1,64,132)\n",
        "  assert currvid.shape==(1,64,132) ,\"Something went wrong\"\n",
        "  #print(trainXfinal.shape,currvid.shape)\n",
        "  testXfinal=np.vstack([testXfinal,currvid])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7RceyRMi3c8",
        "outputId": "2e94a1b9-a5ab-4467-811f-a3da922ee110"
      },
      "source": [
        "print(testXfinal.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(89, 64, 132)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYnvodHllWSs"
      },
      "source": [
        "#defining the model\n",
        "def get_model(num_len,num_features,num_outputs):\n",
        "  model = Sequential()\n",
        "  model.add(TimeDistributed(Conv1D(filters=16, kernel_size=3, kernel_initializer='glorot_uniform',activation='relu'), input_shape=(None,num_len,num_features)))\n",
        "  model.add(TimeDistributed(Conv1D(filters=8, kernel_size=3, activation='relu')))\n",
        "  model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "  model.add(TimeDistributed(Flatten()))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dropout(0.2))\n",
        "  adam=Adam(lr=0.0001)\n",
        "  model.add(Dense(num_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
        "  return model\n",
        "num_ts, num_features, num_outputs = testXfinal.shape[1], testXfinal.shape[2], 6\n",
        "num_step, num_len = 4, 16\n",
        "testXfinal = testXfinal.reshape(testXfinal.shape[0], num_step, num_len, num_features)\n",
        "model=get_model(num_len,num_features,num_outputs)\n",
        "model.load_weights(\"/content/drive/Shareddrives/Major project/Major Project/Human Activity Recognition on Yoga Dataset/weights/val1-83-0.9865.hdf5\")\n",
        "preds = model.predict(testXfinal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ejFs6UZnogw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "540c724c-9b68-4fec-ab50-2b58ee96a6da"
      },
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import skvideo.io\n",
        "import tqdm\n",
        "Y_pred_asanas = np.argmax(preds, axis = 1)\n",
        "answer=np.bincount(Y_pred_asanas).argmax()\n",
        "asanas = {0:'bhujangasan', 1:'padamasan', 2:'shavasan', 3:'tadasan', 4:'trikonasan', 5:'vrikshasan'}\n",
        "print(\"\\nName of asana recognized by the model is \"+asanas[answer])\n",
        "def extract_frames(video_path):\n",
        "    cam=cv.VideoCapture(video_path)\n",
        "    extract_frames.fps=cam.get(cv.CAP_PROP_FPS)\n",
        "    #frame_array=[]\n",
        "    while(True):\n",
        "      ret,frame = cam.read()\n",
        "      if ret:\n",
        "        #frame_array.append(frame)\n",
        "        yield frame\n",
        "      else:\n",
        "        break\n",
        "font_path = \"/content/drive/Shareddrives/Major project/Major Project/RocknRollOne-Regular.ttf\"\n",
        "font = ImageFont.truetype(font_path, 30)\n",
        "writer = skvideo.io.FFmpegWriter(asanas[answer]+\".mp4\",outputdict={\"-pix_fmt\": \"yuv420p\",\"-r\":str(30)})  #defining the imagewriter\n",
        "for frame in tqdm.tqdm(extract_frames(video_path), desc=\"Writing to video\"):\n",
        "  frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "  frame = Image.fromarray(frame)\n",
        "  d = ImageDraw.Draw(frame)\n",
        "  d.text(xy=(60,60), text=asanas[answer], fill=(0, 153, 153),font=font)\n",
        "  writer.writeFrame(np.array(frame))\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Name of asana recognized by the model is bhujangasan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing to video: 1468it [01:25, 17.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUhxAOwKcSnF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}